# -*- coding: utf-8 -*-
"""Welcome To Colaboratory

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/notebooks/welcome.ipynb
"""

import numpy as np
import torch
import sys
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.autograd import Variable
from torch.utils import data
from torchvision import transforms
from torch.utils.data import Dataset, DataLoader
from torch.nn.utils.rnn import *
import matplotlib.pyplot as plt
!pip install python-Levenshtein
from Levenshtein import distance

import ipywidgets
import traitlets
import time

cuda = torch.cuda.is_available()
cuda
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
!nvidia-smi

from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
!git clone --recursive https://github.com/parlance/ctcdecode.git
!pip install wget
# %cd ctcdecode
!pip install .
# %cd ..

!pip install kaggle

!mkdir .kaggle
!touch .kaggle/kaggle.json

api_token = {"username":"abhishekmahajani","key":"61f40fbb8972a45478940ac8f60a73ea"}

import json
import zipfile
import os
with open('/content/.kaggle/kaggle.json', 'w') as file:
    json.dump(api_token, file)

!chmod 600 /content/.kaggle/kaggle.json
!mv .kaggle /root/
!kaggle config path -p /content

!kaggle competitions download -c 11-785-s20-hw3p2

!unzip  -o -q \*.zip

train_label_list = list(np.load('wsj0_train_merged_labels.npy', allow_pickle=True))
train_data_list = list(np.load('wsj0_train',  allow_pickle=True))
dev_data_list = list(np.load('wsj0_dev.npy', allow_pickle=True))
dev_label_list = list(np.load('wsj0_dev_merged_labels.npy', allow_pickle=True))
test = np.load('wsj0_test', allow_pickle=True)
from ctcdecode import CTCBeamDecoder
from phoneme_list import *

class MyDataset(data.Dataset):
    def __init__(self, X, Y):

        self.X = X
        self.Y = Y

    def __len__(self):
        return len(self.Y)

    def __getitem__(self,index):
      X = torch.from_numpy(self.X[index])
      Y = torch.from_numpy(self.Y[index])
      return X,Y

def my_collate(batch):
  data, labels = zip(*batch)
  in_lens = torch.LongTensor([len(i) for i in data])
  target_lens = torch.LongTensor([len(i) for i in labels])
  return pad_sequence(data), pad_sequence(labels, batch_first = True, padding_value = 46), in_lens, target_lens

num_workers = 4
train_dataset = MyDataset(train_data_list, train_label_list)

train_loader_args = dict(shuffle=True, batch_size=64, num_workers=num_workers, pin_memory=True, collate_fn = my_collate)
train_loader = DataLoader(train_dataset, **train_loader_args)

# Testing
test_dataset = MyDataset(dev_data_list, dev_label_list)

test_loader_args = dict(shuffle=False, batch_size=32, num_workers=2, pin_memory=True, collate_fn = my_collate)
test_loader = DataLoader(test_dataset, **test_loader_args)

class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.embed = nn.Embedding(40, len(PHONEME_LIST))
        self.lstm1 = nn.LSTM(input_size=len(PHONEME_LIST), hidden_size=256, num_layers=7, bidirectional=True)
        self.lstm2 = nn.LSTM(input_size=512, hidden_size=256, num_layers=7, bidirectional=True)
        # self.lstm3 = nn.LSTM(input_size=1024, hidden_size=256, num_layers=7, bidirectional=True)
        # self.lstm4 = nn.LSTM(input_size=512, hidden_size=256, num_layers=7, bidirectional=True)
        # self.lstm5 = nn.LSTM(input_size=512, hidden_size=256, num_layers=7, bidirectional=True)
        # self.lstm6 = nn.LSTM(input_size=512, hidden_size=256, num_layers=7, bidirectional=True)
        # self.output1 = nn.Linear(256 * 2, 256)
        # bn1 = nn.BatchNorm1d(256)
        # act1 = nn.LeakyReLU()
        # self.output2 = nn.Linear(256, 256)
        # bn2 = nn.BatchNorm1d(256)
        # act2 = nn.LeakyReLU()
        # self.output3 = nn.Linear(256, 128)
        # bn3 = nn.BatchNorm1d(128)
        # act3 = nn.LeakyReLU()
        self.output4 = nn.Linear(256*2, 47)
    
    def forward(self, X, lengths):
        X = self.embed(torch.LongTensor(X))
        out = pack_padded_sequence(X, lengths, enforce_sorted=False)
        out = out.cuda()
        out = self.lstm1(out)[0]
        out = self.lstm2(out)[0]
        # out = self.lstm3(out)[0]
        # out = self.lstm4(out)[0]
        # out = self.lstm5(out)[0]
        # out = self.lstm6(out)[0]
        out, out_lens = pad_packed_sequence(out)
        # out = F.LeakyReLU(self.bn1(self.output1(x)))
        # out = F.LeakyReLU(self.bn1(self.output2(x)))
        # out = F.LeakyReLU(self.bn1(self.output3(x)))
        out = self.output4(out).log_softmax(2)
        
        return out, out_lens

def train():
  model.train()
  count = 0
  loss_val = 0
  for batch_idx, (data, target, in_lens, target_lens) in enumerate(test_loader):   
    optimizer.zero_grad() 
    data, in_lens = data.to(device), in_lens.to(device)
    out, out_lens = model(data, in_lens)
    loss = criterion(out, target, out_lens, target_lens)
    loss.backward()
    optimizer.step()
    loss_val += loss.item()
    count+=1
  val = loss_val/count
  return val

def val():
  model.eval()
  distances = []
  for batch_idx, (data, target, in_lens, target_lens) in enumerate(test_loader):
    data, in_lens = data.to(device), in_lens.to(device)
    out, out_lens = model(data, in_lens)
    decoder = CTCBeamDecoder(PHONEME_LIST, beam_width=3)
    decoded_out, _, _, decoded_lens = decoder.decode(out.transpose(0, 1).cpu(), out_lens.cpu())
    decoded_strings = [label_to_short_phoneme(decoded_out[i, 0, :decoded_lens[i]]) for i in range(decoded_out.shape[0])]
    decoded_labels = [label_to_short_phoneme(label_pad[i, : target_lens[i]]) for i in range(label_pad.shape[0])]
    batch_distances = [distance(o, l) for o, l in zip(decoded_strings, decoded_labels)]
    distances.extend(batch_distances)
    print('Distance = ', np.mean(distances))

model = Model()
model = model.to(device)
criterion = nn.CTCLoss(blank = 46)
torch.cuda.empty_cache()

learningRate = 1e-2
weightDecay = 5e-5
optimizer = optim.Adam(model.parameters(), lr=0.0001)
scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.95)

decoder = CTCBeamDecoder(PHONEME_LIST * len(PHONEME_LIST), beam_width=4, log_probs_input=True)

running_loss = 0.0
n_epochs = 10
i = 0
for i in range(n_epochs):
  start_time = time.time()
  epoch_loss = train()
  end_time = time.time()
  print('Epoch ', (i + 1), 'Epoch Loss ', epoch_loss, 'Total time ', end_time - start_time)
  val()

